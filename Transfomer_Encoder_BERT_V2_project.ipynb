{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "3f4eedab",
      "metadata": {
        "id": "3f4eedab"
      },
      "source": [
        "**Copyright: Â© NexStream Technical Education, LLC**.  \n",
        "All rights reserved"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a078249",
      "metadata": {
        "id": "9a078249"
      },
      "source": [
        "##Combined BERT Simplified with Classifiers    \n",
        "\n",
        "In this project, you will utilize pre-trained BERT embeddings and traditional Machine Learning models to implement a Sentiment Analyzer.   \n",
        "Specifically, you will:   \n",
        "- Explore how pre-trained BERT embeddings can be used with traditional machine learning models\n",
        "- Compare transformer-based embedding extraction with traditional NLP embedding methods\n",
        "- Implement a sentiment analysis pipeline using BERT embeddings and logistic regression\n",
        "- Evaluate model performance and interpret results using appropriate metrics\n",
        "\n",
        "Follow the instructions in the code cells to complete and test your code. You will replace all triple underscores (___) with your code. Please refer to the lecture slides for details on each of the functions/algorithms and hints on the implementation.   \n",
        "\n",
        "<br>\n",
        "\n",
        "**NOTE - IF USING COLAB, YOU SHOULD SET YOUR RUNTIME TO USE A GPU FOR THIS NOTEBOOK!!!**\n",
        "- Select Runtime - Change runtime type, then select a GPU option\n",
        "- Note that sometimes the free version will prohibit you from using the GPU resources during peak use times"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dbe68ac6",
      "metadata": {
        "id": "dbe68ac6"
      },
      "source": [
        "**Introduction to Transfer Learning with Transformers**   \n",
        "\n",
        "Pre-trained Language Models  \n",
        "Pre-trained transformer models like BERT (Bidirectional Encoder Representations from Transformers) are trained on massive text corpora to learn contextual representations of language, which can then be leveraged for downstream tasks without having to train a deep learning model from scratch.\n",
        "\n",
        "<br>\n",
        "\n",
        "BERT uses the transformer encoder architecture, which consists of:\n",
        "- Multi-head self-attention mechanisms: Allow the model to focus on different parts of the input sequence\n",
        "- Feed-forward neural networks: Process the attended information\n",
        "- Layer normalization and residual connections: Facilitate training of deep networks\n",
        "\n",
        "<br>\n",
        "\n",
        "Transfer Learning Approaches with BERT   \n",
        "There are two primary approaches to using BERT for downstream tasks:\n",
        "- Fine-tuning: Update all or part of BERT's parameters on a target task\n",
        "- Feature extraction via pre-trained models: Use BERT as a fixed feature extractor and train a separate model (e.g. classifier) on these features\n",
        "\n",
        "<br>\n",
        "\n",
        "In this project, we'll focus on the feature extraction (pre-trained model) approach, which is more computationally efficient and often works well when paired with traditional machine learning models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f157592a",
      "metadata": {
        "id": "f157592a"
      },
      "source": [
        "###Step 1.  Install and import the required libraries   \n",
        "\n",
        "No coding needed - simply run this cell to set up your environment.  \n",
        "\n",
        "\n",
        "**NOTE - IF USING COLAB, YOU SHOULD SET YOUR RUNTIME TO USE A GPU FOR FASTER EXECUTION**\n",
        "- Select Runtime - Change runtime type, then select a GPU option\n",
        "- Note that sometimes the free version will prohibit you from using the GPU resources during peak use times\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "929d435f",
      "metadata": {
        "id": "929d435f"
      },
      "outputs": [],
      "source": [
        "#Install and Import Required Libraries\n",
        "!pip install transformers scikit-learn torch\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure TensorFlow to handle GPU memory properly\n",
        "try:\n",
        "    # Configure GPU memory growth to avoid allocation issues\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"GPU Available: {len(gpus)} GPU(s) configured with memory growth\")\n",
        "    else:\n",
        "        print(\"No GPU available, using CPU\")\n",
        "except Exception as e:\n",
        "    print(f\"GPU configuration warning: {e}\")\n",
        "    print(\"Continuing with default configuration\")\n",
        "\n",
        "print(\"PyTorch CUDA Available: \", torch.cuda.is_available())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5cff4eb",
      "metadata": {
        "id": "b5cff4eb"
      },
      "source": [
        "###Step 2:  Load BERT tokenizer  \n",
        "\n",
        "Load the Hugging Face BERT tokenizer using a BERT base pre-trained, uncased model.  \n",
        "Reference links:\n",
        "- https://huggingface.co/docs/transformers/en/model_doc/bert#transformers.BertTokenizer  \n",
        "- https://huggingface.co/google-bert/bert-base-uncased"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf0f3163",
      "metadata": {
        "id": "cf0f3163"
      },
      "outputs": [],
      "source": [
        "#Load the BERT tokenizer using pre-trained model, uncased model\n",
        "\n",
        "print(\"Loading BERT tokenizer...\")\n",
        "tokenizer = ___"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6ccff4d",
      "metadata": {
        "id": "f6ccff4d"
      },
      "source": [
        "###Step 3: BERT Embedding Extraction Function   \n",
        "\n",
        "Tokenization and Input Preparation with the BERT pre-trained model, including the following:  \n",
        "- Tokenization using WordPiece (breaking words into subword units)\n",
        "- Adding special tokens: [CLS] at the beginning and [SEP] at the end\n",
        "- Converting tokens to IDs using BERT's vocabulary\n",
        "- Creating attention masks to handle padding\n",
        "\n",
        "<br>\n",
        "\n",
        "Notes on the [CLS] token:\n",
        "The [CLS] token is a special token added to the beginning of each input sequence. BERT is trained so that the final hidden state corresponding to this token serves as an aggregate representation of the entire sequence, making it ideal for classification tasks.  \n",
        "When BERT processes text, it works in three key ways that enable the [CLS] token to represent the full sequence:\n",
        "- Bidirectional Context: Unlike earlier models that processed text from left-to-right, BERT's transformer architecture allows each word to \"see\" all other words in the sequence. This means the [CLS] token's representation is influenced by every other word in the text.\n",
        "- Self-Attention Mechanism: BERT uses attention mechanisms that allow it to focus on different parts of the input sequence when creating each token's representation. The [CLS] token's representation is shaped by these attention patterns across the entire sequence.\n",
        "- Special Pre-training: During BERT's pre-training, the [CLS] token was specifically trained to serve as an aggregate representation for classification tasks through a \"next sentence prediction\" objective. This trained the model to pack sentence-level information into this token.\n",
        "\n",
        "In this step, you will write the function:  \n",
        "\n",
        "    def extract_bert_embeddings(texts, max_length=128, batch_size=32):\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    texts : list\n",
        "        List of texts to encode\n",
        "    max_length : int\n",
        "        Maximum sequence length for tokenization\n",
        "    batch_size : int\n",
        "        Batch size for processing to manage memory\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    numpy.ndarray\n",
        "        Array of BERT embeddings with shape (n_texts, 768)\n",
        "\n",
        "Reference links:\n",
        "- Initialize PyTorch BERT model:  https://huggingface.co/docs/transformers/v4.50.0/en/model_doc/bert#transformers.BertModel\n",
        "- Note:  Set to evaluation mode using Model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fe9612a",
      "metadata": {
        "id": "2fe9612a"
      },
      "outputs": [],
      "source": [
        "def extract_bert_embeddings(texts, max_length=128, batch_size=32):\n",
        "    \"\"\"\n",
        "    Extract BERT embeddings from text inputs using PyTorch BERT.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    texts : list\n",
        "        List of texts to encode\n",
        "    max_length : int\n",
        "        Maximum sequence length for tokenization\n",
        "    batch_size : int\n",
        "        Batch size for processing to manage memory\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    numpy.ndarray\n",
        "        Array of BERT embeddings with shape (n_texts, 768)\n",
        "    \"\"\"\n",
        "    # Load PyTorch BERT model (more stable than TensorFlow version)\n",
        "    # Set the model to use the BERT base model, uncased.\n",
        "    embedding_model = ___\n",
        "\n",
        "    # Set to evaluation mode for consistency.\n",
        "    # Execute the following line (no coding needed).\n",
        "    # This function disables dropout and batch normalization updates.\n",
        "    embedding_model.eval()\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    # Execute the following lines (no coding needed)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    embedding_model = embedding_model.to(device)\n",
        "\n",
        "    # Function to decode review if it's in list format (from keras dataset)\n",
        "    # Execute the following function (no coding needed)\n",
        "    def decode_review(text):\n",
        "        if not isinstance(text, list):\n",
        "            return text\n",
        "        # If you're using the IMDB dataset from keras\n",
        "        try:\n",
        "            word_index = keras.datasets.imdb.get_word_index()\n",
        "            reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
        "            return ' '.join([reverse_word_index.get(i - 3, '?') for i in text])\n",
        "        except:\n",
        "            return \"Unable to decode review\"\n",
        "\n",
        "    # Process texts in batches to avoid memory issues\n",
        "    # Create empty list to store all embeddings\n",
        "    all_embeddings = ___\n",
        "\n",
        "\n",
        "    # Disable gradient computation for efficiency.\n",
        "    # Execute the following line (no coding needed)\n",
        "    with torch.no_grad():\n",
        "\n",
        "    # Loop over the length of the input texts in steps of the input batch size\n",
        "        for i in range(0, ___, ___):\n",
        "\n",
        "            # Slice the batch of texts\n",
        "            batch_texts = ___\n",
        "\n",
        "            # Create an empty list to store the embeddings.\n",
        "            batch_embeddings = ___\n",
        "\n",
        "            # Loop over the batch of texts\n",
        "            for text in ___:\n",
        "\n",
        "                # Call the decode_review function and pass the text batch\n",
        "                decoded_text = ___\n",
        "\n",
        "                # Convert a string to a sequence of ids (integer), using the tokenizer and vocabulary.\n",
        "                # The 'tokenizer' reference was set up in Step 2.\n",
        "                # Hint: see the text cells for this step for reference API\n",
        "                # Set decoded_text to be the first sequence to be encoded\n",
        "                # Set the maximum length and padding to max_length\n",
        "                # Set truncation to True\n",
        "                # Set return tensors to 'pt' for PyTorch\n",
        "                inputs = ___(\n",
        "                         ___,\n",
        "                         ___,\n",
        "                         ___,\n",
        "                         ___,\n",
        "                         ___\n",
        "                )\n",
        "\n",
        "                # Move inputs to same device as model\n",
        "                # Execute the following line (no coding needed)\n",
        "                inputs = {key: val.to(device) for key, val in inputs.items()}\n",
        "\n",
        "                # Extract embeddings from the embedding_model.\n",
        "                # Here we need to feed the input_ids and attention_mask from the encoded_input to the BertModel.\n",
        "                # Hint:  do this by passing a dictionary from your tokenizer 'inputs' result defined above.\n",
        "                outputs = ___\n",
        "\n",
        "                # Get the [CLS] token embedding (first token)\n",
        "                # Hint:  convert the token to numpy\n",
        "                cls_embedding = ___\n",
        "                batch_embeddings.___\n",
        "\n",
        "            all_embeddings.extend(batch_embeddings)\n",
        "\n",
        "            if i % 500 == 0:\n",
        "                print(f\"Processed {i}/{len(texts)} examples\")\n",
        "\n",
        "    return np.array(all_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "692fe06a",
      "metadata": {
        "id": "692fe06a"
      },
      "source": [
        "###Step 4:  Classifier Building Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81ff161b",
      "metadata": {
        "id": "81ff161b"
      },
      "source": [
        "####Sequential Neural Network Classifier\n",
        "\n",
        "    Build a sequential neural network classifier for BERT embeddings.\n",
        "\n",
        "    - Dense Layer: 256 units with ReLU activation (input dimension = dimension of the BERT base embeddings)\n",
        "    - First Dropout Layer: Helps prevent overfitting (rate=0.2)\n",
        "    - Dense Layer: 64 units with ReLU activation\n",
        "    - Second Dropout Layer: Additional regularization (rate=0.2)\n",
        "    - Classification Layer: 1 unit with sigmoid activation for binary classification\n",
        "\n",
        "    Compile the model with:\n",
        "    - Optimizer: Adam\n",
        "    - Loss: Binary Cross-Entropy\n",
        "    - Metrics: Accuracy\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_dim : int\n",
        "        Dimension of the input embeddings (768 for BERT base)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    keras.Model\n",
        "        The sequential neural network model\n",
        "\n",
        "Reference links:\n",
        "- https://keras.io/api/models/sequential/\n",
        "- https://keras.io/api/layers/core_layers/dense/\n",
        "- https://keras.io/api/models/model_training_apis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23d375d7",
      "metadata": {
        "id": "23d375d7"
      },
      "outputs": [],
      "source": [
        "def build_sequential_neural_network(input_dim=768):\n",
        "    \"\"\"\n",
        "      Build a sequential neural network classifier for BERT embeddings.\n",
        "      - Dense Layer: 256 units with ReLU activation (input dimension = dimension of the BERT base embeddings)\n",
        "      - First Dropout Layer: Helps prevent overfitting (rate=0.2)\n",
        "      - Dense Layer: 64 units with ReLU activation\n",
        "      - Second Dropout Layer: Additional regularization (rate=0.2)\n",
        "      - Classification Layer: 1 unit with sigmoid activation for binary classification\n",
        "\n",
        "      Compile the model with:\n",
        "      - Optimizer: Adam\n",
        "      - Loss: Binary Cross-Entropy\n",
        "      - Metrics: Accuracy\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      input_dim : int\n",
        "          Dimension of the input embeddings (768 for BERT base)\n",
        "\n",
        "      Returns:\n",
        "      --------\n",
        "      keras.Model\n",
        "          The sequential neural network model\n",
        "    \"\"\"\n",
        "\n",
        "    # Build the Sequential network according to the specifications above\n",
        "    model = ___([\n",
        "                  ___,\n",
        "                  ___,\n",
        "                  ___,\n",
        "                  ___,\n",
        "                  ___\n",
        "    ])\n",
        "\n",
        "    # Compile the model according to the specifications above\n",
        "    model.___\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59b7a2a4",
      "metadata": {
        "id": "59b7a2a4"
      },
      "source": [
        "####Logistic Regression Classifier   \n",
        "\n",
        "    \"\"\"\n",
        "    Build a logistic regression classifier for BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    C : float\n",
        "        Inverse of regularization strength\n",
        "    max_iter : int\n",
        "        Maximum number of iterations for solver\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    LogisticRegression\n",
        "        The logistic regression model\n",
        "    \"\"\"\n",
        "\n",
        "Reference links:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76a7fe41",
      "metadata": {
        "id": "76a7fe41"
      },
      "outputs": [],
      "source": [
        "def build_logistic_regression_classifier(C=1.0, max_iter=1000):\n",
        "    \"\"\"\n",
        "    Build a logistic regression classifier for BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    C : float\n",
        "        Inverse of regularization strength\n",
        "    max_iter : int\n",
        "        Maximum number of iterations for solver\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    LogisticRegression\n",
        "        The logistic regression model\n",
        "    \"\"\"\n",
        "    # Create the logistic regression model according to the specifications above\n",
        "    # Set the random state to 42 for reproducibility.\n",
        "    # Set the regularization strength and max number of iterations parameters according to the inputs\n",
        "    # Hint:  set n_jobs=-1 to utilize all available cores\n",
        "    lr_model = ___(___, ___, random_state=42, ___)\n",
        "    return lr_model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b81156c0",
      "metadata": {
        "id": "b81156c0"
      },
      "source": [
        "###Step 5:  Training and Evaluation Functions   \n",
        "\n",
        "In this step you will create models for the classifiers (Sequential Neural Network and Logistic Regression).  \n",
        "Then you will train the model, make predictions and evaluate the model performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2dbbe01",
      "metadata": {
        "id": "f2dbbe01"
      },
      "source": [
        "####Sequential Neural Network   \n",
        "\n",
        "    \"\"\"\n",
        "    Train and evaluate a neural network classifier on BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : numpy.ndarray\n",
        "        Training data embeddings\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : numpy.ndarray\n",
        "        Test data embeddings\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "    epochs : int\n",
        "        Number of training epochs\n",
        "    batch_size : int\n",
        "        Batch size for training\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (model, history) - the trained model and training history\n",
        "    \"\"\"\n",
        "\n",
        "Reference links:\n",
        "- https://keras.io/api/callbacks/early_stopping/\n",
        "- https://keras.io/api/models/model_training_apis/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "631c08df",
      "metadata": {
        "id": "631c08df"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_neural_network(train_data, train_labels, test_data, test_labels, epochs=10, batch_size=32):\n",
        "    \"\"\"\n",
        "    Train and evaluate a neural network classifier on BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : numpy.ndarray\n",
        "        Training data embeddings\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : numpy.ndarray\n",
        "        Test data embeddings\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "    epochs : int\n",
        "        Number of training epochs\n",
        "    batch_size : int\n",
        "        Batch size for training\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (model, history) - the trained model and training history\n",
        "    \"\"\"\n",
        "    # Force CPU usage to avoid device conflicts\n",
        "    print(\"Using CPU for neural network training to avoid GPU compatibility issues...\")\n",
        "\n",
        "    with tf.device('/CPU:0'):\n",
        "        # Build model on CPU\n",
        "        # Call your build function and pass the input dimension as the number of examples (columns)\n",
        "        model = ___\n",
        "\n",
        "        # Enable the early stopping callback.\n",
        "        # Set the patience to 3 (Number of epochs with no improvement after which training will be stopped).\n",
        "        # Set the model to restore the model weights from the epoch with the best value of the monitored quantity.\n",
        "        # Hint: https://keras.io/api/callbacks/early_stopping/\n",
        "        early_stopping = ___\n",
        "\n",
        "        # Train model\n",
        "        # Pass in train_data, train_labels\n",
        "        # Set the train/validation split to 90/10%\n",
        "        # Set the number of epochs to the function input parameter\n",
        "        # Set the batch size to the function input parameter\n",
        "        # Set the early stop callback (parameter set up previously)\n",
        "        # Set the verbose flag to '1'\n",
        "        # Hint: https://keras.io/api/models/model_training_apis/\n",
        "        # Hint: call the fit method with the parameters defined above.\n",
        "        history = model.___(\n",
        "                        ___,\n",
        "                        ___,\n",
        "                        ___,\n",
        "                        ___,\n",
        "                        ___,\n",
        "                        ___\n",
        "        )\n",
        "\n",
        "        # Evaluate model\n",
        "        # Hint: https://keras.io/api/models/model_training_apis/\n",
        "        # Hint: call the evaluate method\n",
        "        test_loss, test_accuracy = ___\n",
        "        print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "        # Make predictions\n",
        "        # Hint: https://keras.io/api/models/model_training_apis/\n",
        "        # Hint: call the predict method\n",
        "        predictions = ___\n",
        "\n",
        "        # Scale the prediction labels to 0 or 1 with '1' if the prediction is > 0.5\n",
        "        # Hint: you'll need to flatten() the labels after scaling\n",
        "        predicted_labels = ___\n",
        "\n",
        "\n",
        "    # Generate a classification report.\n",
        "    # Execute the following lines (no coding needed).\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, predicted_labels, target_names=['Negative', 'Positive']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(test_labels, predicted_labels)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix - Neural Network')\n",
        "    plt.show()\n",
        "\n",
        "    # Plot training history\n",
        "    plt.figure(figsize=(12, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'])\n",
        "    plt.plot(history.history['val_accuracy'])\n",
        "    plt.title('Neural Network Accuracy')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'])\n",
        "    plt.plot(history.history['val_loss'])\n",
        "    plt.title('Neural Network Loss')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.legend(['Train', 'Validation'], loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return model, history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c6af7e96",
      "metadata": {
        "id": "c6af7e96"
      },
      "source": [
        "####Logistic Regression   \n",
        "\n",
        "    \"\"\"\n",
        "    Train and evaluate a logistic regression classifier on BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : numpy.ndarray\n",
        "        Training data embeddings\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : numpy.ndarray\n",
        "        Test data embeddings\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    LogisticRegression\n",
        "        The trained logistic regression model\n",
        "    \"\"\"\n",
        "\n",
        "Reference links:\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict\n",
        "- https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a819e73",
      "metadata": {
        "id": "0a819e73"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate_logistic_regression(train_data, train_labels, test_data, test_labels):\n",
        "    \"\"\"\n",
        "    Train and evaluate a logistic regression classifier on BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : numpy.ndarray\n",
        "        Training data embeddings\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : numpy.ndarray\n",
        "        Test data embeddings\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    LogisticRegression\n",
        "        The trained logistic regression model\n",
        "    \"\"\"\n",
        "    # Build the logistic regression model\n",
        "    # Call your build function\n",
        "    model = ___\n",
        "\n",
        "    # Train model\n",
        "    # Pass in train_data, train_labels\n",
        "    # Hint: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit\n",
        "    print(\"Training logistic regression model...\")\n",
        "    model.___\n",
        "\n",
        "    # Evaluate model\n",
        "    # Hint: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.score\n",
        "    train_accuracy = ___\n",
        "    test_accuracy = ___\n",
        "\n",
        "    print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
        "\n",
        "    # Make the following predictions:\n",
        "    #     1. predicted class labels for the test data (test_data)\n",
        "    #     2. probability estimates for each class for the test data (store the probability of a positive class only)\n",
        "    # Hint:  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict\n",
        "    # Hint:  https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.predict_proba\n",
        "    predictions = ___\n",
        "    predicted_proba = ___  # Probability of positive class\n",
        "\n",
        "    # Generate a classification report\n",
        "    # Execute the following lines (no coding needed)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(test_labels, predictions, target_names=['Negative', 'Positive']))\n",
        "\n",
        "    # Confusion matrix\n",
        "    cm = confusion_matrix(test_labels, predictions)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['Negative', 'Positive'],\n",
        "                yticklabels=['Negative', 'Positive'])\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.ylabel('Actual')\n",
        "    plt.title('Confusion Matrix - Logistic Regression')\n",
        "    plt.show()\n",
        "\n",
        "    # Feature importance analysis (coefficients)\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    # Sort coefficients and plot top 20 most influential\n",
        "    coef = model.coef_[0]\n",
        "    top_positive_idx = np.argsort(coef)[-20:]\n",
        "    top_negative_idx = np.argsort(coef)[:20]\n",
        "\n",
        "    plt.barh(range(20), coef[top_positive_idx], color='green')\n",
        "    plt.barh(range(20, 40), coef[top_negative_idx], color='red')\n",
        "    plt.yticks([])\n",
        "    plt.title('Top 20 Feature Coefficients (Positive and Negative)')\n",
        "    plt.xlabel('Coefficient Value')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # ROC curve\n",
        "    fpr, tpr, _ = roc_curve(test_labels, predicted_proba)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('ROC Curve - Logistic Regression')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f47fd87",
      "metadata": {
        "id": "9f47fd87"
      },
      "source": [
        "###Step 6:  Pipeline Functions   \n",
        "\n",
        "In this step, you will implement the processing pipeline for our BERT and classifiers (Sequential Neural Network and Logistic Regression) model applied to sentiment analysis.   \n",
        "\n",
        "Our sentiment analysis pipeline consists of:\n",
        "- Extracting embeddings from text data from the pre-trained BERT model\n",
        "- Training the sequential neural network and logistic regression model on these embeddings\n",
        "- Evaluate the models performance\n",
        "- Using the models to predict sentiment of new texts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "874c813e",
      "metadata": {
        "id": "874c813e"
      },
      "source": [
        "####Sequential Neural Network   \n",
        "\n",
        "    \"\"\"\n",
        "    Run the complete neural network pipeline on BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : list\n",
        "        Training data texts\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : list\n",
        "        Test data texts\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "    max_length : int\n",
        "        Maximum sequence length for tokenization\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (model, embeddings) - the trained model and BERT embeddings\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad4440ef",
      "metadata": {
        "id": "ad4440ef"
      },
      "outputs": [],
      "source": [
        "def run_neural_network_pipeline(train_data, train_labels, test_data, test_labels, max_length=128):\n",
        "    \"\"\"\n",
        "    Run the complete neural network pipeline on BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : list\n",
        "        Training data texts\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : list\n",
        "        Test data texts\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "    max_length : int\n",
        "        Maximum sequence length for tokenization\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (model, embeddings) - the trained model and BERT embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the BERT embeddings for the training and validation (test) data using your previous function (extract_bert_embeddings)\n",
        "    print(\"Extracting BERT embeddings for training data...\")\n",
        "    train_embeddings = ___\n",
        "    print(\"Extracting BERT embeddings for test data...\")\n",
        "    test_embeddings = ___\n",
        "\n",
        "    # Train and evaluate your model using your previous function (train_and_evaluate_neural_network)\n",
        "    print(\"Training the neural network classifier...\")\n",
        "    model, history = ___\n",
        "\n",
        "    return model, (train_embeddings, test_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f534619",
      "metadata": {
        "id": "9f534619"
      },
      "source": [
        "####Logistic Regression   \n",
        "\n",
        "    \"\"\"\n",
        "    Run the complete logistic regression pipeline on BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : list\n",
        "        Training data texts\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : list\n",
        "        Test data texts\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "    max_length : int\n",
        "        Maximum sequence length for tokenization\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (model, embeddings) - the trained model and BERT embeddings\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ee25db",
      "metadata": {
        "id": "48ee25db"
      },
      "outputs": [],
      "source": [
        "def run_logistic_regression_pipeline(train_data, train_labels, test_data, test_labels, max_length=128):\n",
        "    \"\"\"\n",
        "    Run the complete logistic regression pipeline on BERT embeddings.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : list\n",
        "        Training data texts\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : list\n",
        "        Test data texts\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "    max_length : int\n",
        "        Maximum sequence length for tokenization\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (model, embeddings) - the trained model and BERT embeddings\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the BERT embeddings for the training and validation (test) data using your previous function (extract_bert_embeddings)\n",
        "    print(\"Extracting BERT embeddings for training data...\")\n",
        "    train_embeddings = ___\n",
        "    print(\"Extracting BERT embeddings for test data...\")\n",
        "    test_embeddings = ___\n",
        "\n",
        "    # Train and evaluate your model using your previous function (train_and_evaluate_neural_network)\n",
        "    print(\"Training the logistic regression classifier...\")\n",
        "    model = ___\n",
        "\n",
        "    return model, (train_embeddings, test_embeddings)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86d2e678",
      "metadata": {
        "id": "86d2e678"
      },
      "source": [
        "###Step 7:  Combined Pipeline Function (for efficiency)\n",
        "\n",
        "Hopefully, you recognized that since we are using a pre-trained BERT model, we should be able to make the processing pipeline more efficient by reusing the embeddings for both classifiers.  \n",
        "In this step, you will implement the combine the processing pipeline for our BERT and classifiers (Sequential Neural Network and Logistic Regression) model applied to sentiment analysis.  By taking advantage of the BERT pre-trained model, we can use the same embeddings for both our classifiers.   \n",
        "\n",
        "\n",
        "    \"\"\"\n",
        "    Run both neural network and logistic regression pipelines on the same BERT embeddings.\n",
        "    \n",
        "    This is more efficient as embeddings are extracted only once.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : list\n",
        "        Training data texts\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : list\n",
        "        Test data texts\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "    max_length : int\n",
        "        Maximum sequence length for tokenization\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (nn_model, lr_model, embeddings)\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "133f2de3",
      "metadata": {
        "id": "133f2de3"
      },
      "outputs": [],
      "source": [
        "def run_combined_pipeline(train_data, train_labels, test_data, test_labels, max_length=128):\n",
        "    \"\"\"\n",
        "    Run both neural network and logistic regression pipelines on the same BERT embeddings.\n",
        "\n",
        "    This is more efficient as embeddings are extracted only once.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    train_data : list\n",
        "        Training data texts\n",
        "    train_labels : numpy.ndarray\n",
        "        Training labels\n",
        "    test_data : list\n",
        "        Test data texts\n",
        "    test_labels : numpy.ndarray\n",
        "        Test labels\n",
        "    max_length : int\n",
        "        Maximum sequence length for tokenization\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    tuple\n",
        "        (nn_model, lr_model, embeddings)\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the BERT embeddings for the training and validation (test) data using your previous function (extract_bert_embeddings)\n",
        "    print(\"Extracting BERT embeddings for training data...\")\n",
        "    train_embeddings = ___\n",
        "    print(\"Extracting BERT embeddings for test data...\")\n",
        "    test_embeddings = ___\n",
        "\n",
        "    # Train and evaluate your model using your previous function (train_and_evaluate_neural_network)\n",
        "    # Train neural network\n",
        "    print(\"\\n=== Training Neural Network Classifier ===\")\n",
        "    nn_model, history = ___\n",
        "\n",
        "    # Train logistic regression\n",
        "    print(\"\\n=== Training Logistic Regression Classifier ===\")\n",
        "    lr_model = ___\n",
        "\n",
        "    return nn_model, lr_model, (train_embeddings, test_embeddings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a2d660e",
      "metadata": {
        "id": "1a2d660e"
      },
      "source": [
        "###Step 8:  Prediction Functions   \n",
        "\n",
        "Now we are able to test our sentiment analyzer by making predictions with our pre-trained BERT and classifier (Sequential Neural Network or Logistic Regression) models.\n",
        "\n",
        "Write a function which inputs a text, model reference and length and returns analytics.\n",
        "\n",
        "    def predict_sentiment_nn(text, model, max_length=128):\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bedccdca",
      "metadata": {
        "id": "bedccdca"
      },
      "source": [
        "####Sequential Neural Network   \n",
        "\n",
        "    \"\"\"\n",
        "    Predict sentiment using the neural network model.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "99aac0ed",
      "metadata": {
        "id": "99aac0ed"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment_nn(text, model, max_length=128):\n",
        "    \"\"\"\n",
        "    Predict sentiment using the neural network model.\n",
        "    \"\"\"\n",
        "    # Extract the embeddings for the text using your previous function\n",
        "    embedding = ___\n",
        "\n",
        "    # Make prediction using CPU to match model device\n",
        "    # Hint:  call the predict function for the input model\n",
        "    with tf.device('/CPU:0'):\n",
        "        prediction = ___\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'score': float(prediction),\n",
        "        'sentiment': 'Positive' if prediction > 0.5 else 'Negative',\n",
        "        'confidence': float(prediction) if prediction > 0.5 else float(1 - prediction)\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d1de790",
      "metadata": {
        "id": "7d1de790"
      },
      "source": [
        "####Logistic Regression   \n",
        "\n",
        "    \"\"\"\n",
        "    Predict sentiment using the logistic regression model.\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "39414f46",
      "metadata": {
        "id": "39414f46"
      },
      "outputs": [],
      "source": [
        "def predict_sentiment_lr(text, model, max_length=128):\n",
        "    \"\"\"\n",
        "    Predict sentiment using the logistic regression model.\n",
        "    \"\"\"\n",
        "    # Get embedding for the text using your previous function\n",
        "    embedding = ___\n",
        "\n",
        "    # Make prediction\n",
        "    # Hint:  call the predict function for the input model\n",
        "    prediction = model.___\n",
        "    probability = model.___  # Probability of positive class\n",
        "\n",
        "    return {\n",
        "        'text': text,\n",
        "        'score': float(probability),\n",
        "        'sentiment': 'Positive' if prediction == 1 else 'Negative',\n",
        "        'confidence': float(probability) if prediction == 1 else float(1 - probability)\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bba375a1",
      "metadata": {
        "id": "bba375a1"
      },
      "source": [
        "###Step 9:  Load and Prepare Data   \n",
        "\n",
        "No coding needed for this step.   \n",
        "Do not modify this cell - for test purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8e15bbf",
      "metadata": {
        "id": "b8e15bbf"
      },
      "outputs": [],
      "source": [
        "# Load IMDB dataset\n",
        "imdb = keras.datasets.imdb\n",
        "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
        "\n",
        "# Create a smaller sample for demonstration\n",
        "sample_size = 5000  # Adjust based on available computational resources\n",
        "train_sample = train_data[:sample_size]\n",
        "train_sample_labels = train_labels[:sample_size]\n",
        "test_sample = test_data[:1000]\n",
        "test_sample_labels = test_labels[:1000]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46663824",
      "metadata": {
        "id": "46663824"
      },
      "source": [
        "###Step 10:  Run the combined pipeline   \n",
        "\n",
        "No coding needed for this step.   \n",
        "Do not modify this cell - for test purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0f76ae5",
      "metadata": {
        "id": "c0f76ae5"
      },
      "outputs": [],
      "source": [
        "nn_model, lr_model, embeddings = run_combined_pipeline(\n",
        "    train_sample, train_sample_labels,\n",
        "    test_sample, test_sample_labels\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7023880",
      "metadata": {
        "id": "b7023880"
      },
      "source": [
        "###Step 11:  Model Comparison  \n",
        "\n",
        "No coding needed for this step.   \n",
        "Do not modify this cell - for test purposes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69874baa",
      "metadata": {
        "id": "69874baa"
      },
      "outputs": [],
      "source": [
        "# Try some examples with both models\n",
        "examples = [\n",
        "    \"This movie was fantastic! I really enjoyed every minute of it.\",\n",
        "    \"The acting was terrible and the plot made no sense.\",\n",
        "    \"It was okay, not great but not terrible either.\"\n",
        "]\n",
        "\n",
        "print(\"===== Model Comparison =====\")\n",
        "for i, example in enumerate(examples):\n",
        "    nn_result = predict_sentiment_nn(example, nn_model)\n",
        "    lr_result = predict_sentiment_lr(example, lr_model)\n",
        "\n",
        "    print(f\"\\nExample {i+1}: \\\"{example}\\\"\")\n",
        "    print(f\"Neural Network: {nn_result['sentiment']} (confidence: {nn_result['confidence']:.2f})\")\n",
        "    print(f\"Logistic Regression: {lr_result['sentiment']} (confidence: {lr_result['confidence']:.2f})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a34ef3d6",
      "metadata": {
        "id": "a34ef3d6"
      },
      "source": [
        "###Step 12:  (Optional): Parameter Sensitivity Analysis for Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1550fe59",
      "metadata": {
        "id": "1550fe59"
      },
      "outputs": [],
      "source": [
        "# Tune the Logistic Regression regularization parameter and observe its effect on performance\n",
        "\n",
        "print(\"\\n===== Logistic Regression Parameter Sensitivity Analysis =====\")\n",
        "c_values = [0.01, 0.1, 1.0, 10.0, 100.0]\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "for c in c_values:\n",
        "    lr_model = LogisticRegression(C=c, max_iter=1000, random_state=42)\n",
        "    lr_model.fit(embeddings[0], train_sample_labels)\n",
        "\n",
        "    train_acc = lr_model.score(embeddings[0], train_sample_labels)\n",
        "    test_acc = lr_model.score(embeddings[1], test_sample_labels)\n",
        "\n",
        "    train_accuracies.append(train_acc)\n",
        "    test_accuracies.append(test_acc)\n",
        "\n",
        "    print(f\"C={c}: Train Accuracy={train_acc:.4f}, Test Accuracy={test_acc:.4f}\")\n",
        "\n",
        "# Plot regularization parameter sensitivity\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.semilogx(c_values, train_accuracies, 'b-o', label='Training Accuracy')\n",
        "plt.semilogx(c_values, test_accuracies, 'r-o', label='Test Accuracy')\n",
        "plt.xlabel('Regularization Parameter (C)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Logistic Regression Sensitivity to Regularization')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b001df",
      "metadata": {
        "id": "05b001df"
      },
      "source": [
        "## REFLECTION QUESTIONS  \n",
        "\n",
        "Answer the following questions.\n",
        "\n",
        "1. Summarize the performance of both your models.  What explains the difference in training accuracy vs. test accuracy between the neural network and logistic regression models?\n",
        "2. Why might the models disagree on the neutral example \"It was okay, not great but not terrible either\"? Why does the neural network appear \"overconfident\" (high confidence score in its sentiment prediction)?\n",
        "3. Given the similar performance of both models, what factors would influence your choice between using the neural network or logistic regression in a production environment?\n",
        "4. How does the feature extraction approach using BERT embeddings differ from traditional NLP approaches, and why is it particularly effective for this task?\n",
        "5. What modifications would you make to either model to improve performance on neutral or ambiguous examples?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}